{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn==1.1 --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# fix_seed(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터는 연속적이지가 않고, 뚝뚝 끊어지는 현상이 있음.\n",
        "sc = MinMaxScaler()\n",
        "file = \"../../data/RAW_EEG/sub-01/sub-01_ses-01.csv\"\n",
        "tmp = pd.read_csv(file)\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Line(x=tmp.index, y=tmp[\"A1\"], name=\"A1\"))\n",
        "fig.add_trace(go.Line(x=tmp.index, y=tmp[\"event\"]*100, name=\"event\"))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 컬럼별 범위 다름\n",
        "tmp.agg([\"min\",\"max\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파일별로 범위도 다름\n",
        "pd.read_csv(\"../../data/RAW_EEG/sub-01/sub-01_ses-01.csv\")[\"A1\"].plot()\n",
        "pd.read_csv(\"../../data/RAW_EEG/sub-01/sub-01_ses-02.csv\")[\"A1\"].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파일간 변화량 비교\n",
        "pd.read_csv(\"../../data/RAW_EEG/sub-01/sub-01_ses-01.csv\")[\"A2\"].diff(1).plot()\n",
        "pd.read_csv(\"../../data/RAW_EEG/sub-01/sub-01_ses-02.csv\")[\"A2\"].diff(1).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 각 컬럼별로 변화량을 보면 이상치가 존재함, -50~50정도를 정상치의 기준으로 잡는것이 좋아보임.\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
        "sns.boxplot(tmp[\"A1\"].diff(1), ax=axes[0][0])\n",
        "sns.boxplot(tmp[\"A2\"].diff(1), ax=axes[0][1])\n",
        "sns.boxplot(tmp[\"A3\"].diff(1), ax=axes[0][2])\n",
        "sns.boxplot(tmp[\"A4\"].diff(1), ax=axes[1][0])\n",
        "sns.boxplot(tmp[\"A5\"].diff(1), ax=axes[1][1])\n",
        "sns.boxplot(tmp[\"A6\"].diff(1), ax=axes[1][2])\n",
        "sns.boxplot(tmp[\"A7\"].diff(1), ax=axes[2][0])\n",
        "sns.boxplot(tmp[\"A8\"].diff(1), ax=axes[2][1])\n",
        "sns.boxplot(tmp[\"A9\"].diff(1), ax=axes[2][2])\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 데이터 변화량으로 변환\n",
        "tmp.iloc[:,:-1] = tmp.iloc[:,:-1].diff(1).dropna().reset_index(drop=True)\n",
        "tmp[\"A1\"].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 각 컬럼별로 이상치 제거후 minmax 정규화하여 다시 변화량 확인\n",
        "sc = MinMaxScaler()\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
        "\n",
        "\n",
        "sns.boxplot(tmp[\"A1\"].diff(1), ax=axes[0][0])\n",
        "sns.boxplot(tmp[\"A2\"].diff(1), ax=axes[0][1])\n",
        "sns.boxplot(tmp[\"A3\"].diff(1), ax=axes[0][2])\n",
        "sns.boxplot(tmp[\"A4\"].diff(1), ax=axes[1][0])\n",
        "sns.boxplot(tmp[\"A5\"].diff(1), ax=axes[1][1])\n",
        "sns.boxplot(tmp[\"A6\"].diff(1), ax=axes[1][2])\n",
        "sns.boxplot(tmp[\"A7\"].diff(1), ax=axes[2][0])\n",
        "sns.boxplot(tmp[\"A8\"].diff(1), ax=axes[2][1])\n",
        "sns.boxplot(tmp[\"A9\"].diff(1), ax=axes[2][2])\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' \n",
        "    0. 파일 읽기\n",
        "    1. (레이블 제외)데이터 변화량으로 변환\n",
        "    2. 이상치 제거\n",
        "    3. 파일별, 컬럼별 데이터 정규화(StandardScaler)\n",
        "    4. concat 한 파일에 대해서 MinMax 정규화\n",
        "'''\n",
        "df = None\n",
        "for csv in [\"../../data/RAW_EEG/sub-04/sub-04_ses-01.csv\", \"../../data/RAW_EEG/sub-04/sub-04_ses-02.csv\", \"../../data/RAW_EEG/sub-04/sub-04_ses-03.csv\"]:\n",
        "    # 0. 파일 읽기\n",
        "    tmp_df = pd.read_csv(csv)\n",
        "    index = tmp_df[\"A1\"]>-999999999\n",
        "    tmp_df = tmp_df[index]\n",
        "    \n",
        "    if df is None:\n",
        "        df = tmp_df\n",
        "    else:\n",
        "        df = pd.concat([df, tmp_df])\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_lengths = []#234440, 234440, 119140 파일의 첫시작은 3841\n",
        "tmp = df.event.diff(1).dropna()\n",
        "seq_lengths = [0] + tmp[tmp!=0].index.to_list()\n",
        "seq_lengths = torch.tensor(seq_lengths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diff_seq_lengths = seq_lengths[1:] - seq_lengths[:-1]\n",
        "seq_indices = torch.stack([seq_lengths[:-1], diff_seq_lengths], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(seq_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_indices = seq_indices[seq_indices[:,-1] % 1153 == 0]\n",
        "len(seq_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = []\n",
        "for index, length in seq_indices:\n",
        "    num_parts = length // 1153  # Determine the number of 1153 segments\n",
        "    for i in range(num_parts):\n",
        "        result.append([index + i * 1153, 1153])  # Append each part to the result list\n",
        "        # result.append([index + i * 1153 + 1, 1152])  # Append each part to the result list\n",
        "seq_indices = torch.tensor(result)  # Convert the result list back to a tensor\n",
        "np_seq_indices = seq_indices.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "window_size = 1\n",
        "\n",
        "# df_list_means = []\n",
        "df_list_diff = []\n",
        "df_list_y = []\n",
        "cur_idx = 0\n",
        "\n",
        "for i in range(len(np_seq_indices)):\n",
        "    start = np_seq_indices[i][0]\n",
        "    end = start + np_seq_indices[i][1]\n",
        "\n",
        "    # means = df.iloc[start:end, :-1].rolling(window_size, min_periods=1).mean()\n",
        "    diff = df.iloc[start:end, :-1].diff(window_size)\n",
        "    y = df.iloc[start:end, -1:]\n",
        "\n",
        "    # means = means.iloc[window_size:]\n",
        "    diff = diff.iloc[window_size:]\n",
        "    y = y.iloc[window_size:]\n",
        "    \n",
        "    add_len = (end - start - window_size)\n",
        "    np_seq_indices[i] = (cur_idx, add_len)\n",
        "    cur_idx = cur_idx + add_len\n",
        "\n",
        "    # df_list_means.append(means)\n",
        "    df_list_diff.append(diff)\n",
        "    df_list_y.append(y)\n",
        "\n",
        "# df_means = pd.concat(df_list_means)\n",
        "df_diff = pd.concat(df_list_diff)\n",
        "df_y = pd.concat(df_list_y)\n",
        "\n",
        "new_column_names = ['diff_' + name for name in df.columns[:-1]]\n",
        "df_diff.columns = new_column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined = pd.concat([df_diff, df_y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined.iloc[:, :-1] = df_combined.iloc[:, :-1].div(df_combined.iloc[:, :-1].abs().mean()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_seq_size = seq_indices[:,1].max()\n",
        "min_len = seq_indices[:,1].min()\n",
        "print(max_seq_size)\n",
        "print(min_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def stratified_sequence_train_test_split(seq_indices, y, test_size=0.2, shuffle=True, random_state=None):\n",
        "    # Get unique labels\n",
        "    unique_labels = np.unique([y[seq_idx] for seq_idx, _ in seq_indices])\n",
        "\n",
        "    train_seq_indices = []\n",
        "    test_seq_indices = []\n",
        "\n",
        "    # Loop over each unique label\n",
        "    for label in unique_labels:\n",
        "        # Find sequences of current label\n",
        "        label_seq_indices = [seq_idx for seq_idx in seq_indices if y[seq_idx[0]] == label]\n",
        "\n",
        "        # Perform train-test split for the current label sequences\n",
        "        train_seq_indices_label, test_seq_indices_label = train_test_split(label_seq_indices, \n",
        "                                                                           test_size=test_size, \n",
        "                                                                           shuffle=shuffle,\n",
        "                                                                           random_state=random_state)\n",
        "\n",
        "        # Append split indices to the main lists\n",
        "        train_seq_indices += train_seq_indices_label\n",
        "        test_seq_indices += test_seq_indices_label\n",
        "\n",
        "    return train_seq_indices, test_seq_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the function\n",
        "X, y = df_combined.iloc[:,:-1].values, df_combined.iloc[:,-1].values\n",
        "train_seq_indices, test_seq_indices = stratified_sequence_train_test_split(seq_indices, y, test_size=0.2, shuffle=False)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "path_append = \"../\"\n",
        "sys.path.append(path_append)  # Go up one directory from where you are.\n",
        "import torch\n",
        "from ccnets.config import get_parser\n",
        "from ccnets.ccnets import CCNets\n",
        "from ccnets.resnets import ResNets\n",
        "from ccnets.utils.loader import save_dataset, load_dataset\n",
        "from nn.transformer import TFEncoder, TFDecoder\n",
        "from ccnets.utils.log import create_log_details, create_log_name\n",
        "from ccnets.utils.setting import set_random_seed\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y, seq_lengths):\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.min_len = min_len\n",
        "        for i in range(len(seq_lengths)):\n",
        "            x_seq = x[seq_lengths[i][0]: seq_lengths[i][0]+seq_lengths[i][1]]\n",
        "            y_seq = y[seq_lengths[i][0]: seq_lengths[i][0]+seq_lengths[i][1]]\n",
        "            self.x.append(x_seq)\n",
        "            self.y.append(y_seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = get_parser()\n",
        "args.device = torch.device('cuda:0' if (torch.cuda.is_available() and args.ngpu > 0) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython ; file_path = IPython.extract_module_locals()[1]['__vsc_ipynb_file__']\n",
        "from pathlib import Path\n",
        "file_name = Path(file_path).stem\n",
        "model_path = path_append + f\"models/{file_name}/\"\n",
        "temp_path = path_append + f\"models/{'temp_'}{file_name}/\"\n",
        "log_path = path_append + f\"log/{file_name}/\"\n",
        "\n",
        "if Path(temp_path).exists() is False: \n",
        "    os.mkdir(temp_path)\n",
        "\n",
        "if Path(model_path).exists() is False: \n",
        "    os.mkdir(model_path)\n",
        "\n",
        "if Path(log_path).exists() is False: \n",
        "    os.mkdir(log_path)  \n",
        "    \n",
        "args.model_path = model_path\n",
        "args.temp_path = temp_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = path_append + f\"data/custom_dataset/{file_name}/\"\n",
        "load_data = False\n",
        "trainset = None\n",
        "testset = None\n",
        "if not os.path.isdir(data_path) or not load_data:\n",
        "    trainset = SequenceDataset(X, y, train_seq_indices)\n",
        "    testset = SequenceDataset(X, y, test_seq_indices)\n",
        "    save_dataset(trainset, testset, data_path)\n",
        "else:\n",
        "    trainset, testset = load_dataset(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args.num_epoch = 5000\n",
        "args.batch_size = 64\n",
        "args.step_size = 20\n",
        "\n",
        "args.num_layer = 3\n",
        "args.hidden_size = 256 \n",
        "args.lr = 2e-4\n",
        "\n",
        "args.obs_size = 128\n",
        "args.label_size = 14\n",
        "args.explain_size = 14\n",
        "args.seq_len = 60\n",
        "\n",
        "args.num_checkpoints = 50\n",
        "args.use_one_hot = True\n",
        "\n",
        "args.use_reasoner_swap_inputs = False\n",
        "args.use_producer_swap_inputs = True    \n",
        "\n",
        "args.reasoner_joint_type = \"none\"\n",
        "args.producer_joint_type = \"none\"\n",
        "\n",
        "args.label_type = \"UC\" \n",
        "\n",
        "args.obs_fn = \"none\"\n",
        "args.label_fn = \"softmax\"\n",
        "\n",
        "args.use_report = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args.loss_type = \"MSE\"\n",
        "args.loss_reduction = \"all\"\n",
        "\n",
        "log_details = create_log_details(args)\n",
        "args.log = SummaryWriter(log_dir=create_log_name(log_path, log_details))\n",
        "\n",
        "set_random_seed(0)\n",
        "resnets = ResNets(args, TFEncoder, TFDecoder)\n",
        "resnets.train(trainset, testset = testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args.loss_type = \"L1\"\n",
        "args.error_type = \"Sub\" \n",
        "args.loss_reduction = \"all\"\n",
        "args.error_reduction = \"none\"\n",
        "\n",
        "log_details = create_log_details(args)\n",
        "args.log = SummaryWriter(log_dir=create_log_name(log_path, log_details))\n",
        "\n",
        "set_random_seed(0)\n",
        "ccnets = CCNets(args, TFEncoder, TFDecoder, TFDecoder)\n",
        "# ccnets.load_models()\n",
        "ccnets.train(trainset, testset = testset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "b287f80b48e4412a59791e63d64f0b079e04f47b5726df5f54fb3b5044d29a99"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
